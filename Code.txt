FORK of [ https://www.kaggle.com/stefankahl/birdclef2021-model-training ].
Inference notebook [ https://www.kaggle.com/virajkadam/birdclef-inference ]

Model training¶
In this notebook, we will train our first model and apply this model to a soundscape. We will keep the amount of training samples, species and soundscapes to a minimum to keep the execution time as short as possible. Remember, this is only a sample implementation, feel free to explore your own workflow.

These are the steps that we will cover:

select audio files we want to use for training
extract spectrograms from those files and save them in a working directory
load selected samples into a large in-memory dataset
build a simple beginners CNN
train the model
apply the model to a selected soundscape and look at the results
1. Settings and imports
Let’s begin with imports and a few basic settings.

import os

import warnings
warnings.filterwarnings(action='ignore')

import pandas as pd
import librosa
import numpy as np
import pickle

from sklearn.utils import shuffle
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow_addons.metrics import F1Score
# Global vars
RANDOM_SEED = 1337
SAMPLE_RATE = 32000
SIGNAL_LENGTH = 5 # seconds
SPEC_SHAPE = (48, 128) # height x width
FMIN = 500
FMAX = 12500
# MAX_AUDIO_FILES = 10000
EPOCHS=50
2. Data preparation
Taking audio files rated >=4 and having more than 175 samples.

# Code adapted from: 
# https://www.kaggle.com/frlemarchand/bird-song-classification-using-an-efficientnet
# Make sure to check out the entire notebook.

# Load metadata file
train = pd.read_csv('../input/birdclef-2021/train_metadata.csv',)

# Limit the number of training samples and classes
# First, only use high quality samples
train = train.query('rating>=4')

# Second, assume that birds with the most training samples are also the most common
# A species needs at least 200 recordings with a rating above 4 to be considered common
birds_count = {}
for bird_species, count in zip(train.primary_label.unique(), 
                               train.groupby('primary_label')['primary_label'].count().values):
    birds_count[bird_species] = count
most_represented_birds = [key for key,value in birds_count.items() if value >= 175] 

TRAIN = train.query('primary_label in @most_represented_birds')
LABELS = sorted(TRAIN.primary_label.unique())

# Let's see how many species and samples we have left
print('NUMBER OF SPECIES IN TRAIN DATA:', len(LABELS))
print('NUMBER OF SAMPLES IN TRAIN DATA:', len(TRAIN))
print('LABELS:', most_represented_birds)
NUMBER OF SPECIES IN TRAIN DATA: 40
NUMBER OF SAMPLES IN TRAIN DATA: 10955
LABELS: ['amerob', 'azaspi1', 'banana', 'barswa', 'bcnher', 'bewwre', 'blujay', 'bncfly', 'carwre', 'compau', 'comrav', 'comyel', 'cubthr', 'daejun', 'ducfly', 'eursta', 'gbwwre1', 'grasal1', 'grekis', 'grhowl', 'houfin', 'houspa', 'houwre', 'lobgna5', 'mallar3', 'marwre', 'norcar', 'normoc', 'redcro', 'rewbla', 'roahaw', 'rubpep1', 'rucspa1', 'sonspa', 'spotow', 'swathr', 'wbwwre1', 'wesmea', 'whimbr', 'yeofly1']
# saving labels 
with open('LABELS.pkl','wb') as f:
    pickle.dump(LABELS,f)
extracting spectrograms
# Shuffle the training data and limit the number of audio files to MAX_AUDIO_FILES
TRAIN = shuffle(TRAIN, random_state=RANDOM_SEED)

# Define a function that splits an audio file, 
# extracts spectrograms and saves them in a working directory
def get_spectrograms(filepath, primary_label, output_dir):
    
    # Open the file with librosa (limited to the first 15 seconds)
    sig, rate = librosa.load(filepath, sr=SAMPLE_RATE, offset=None, duration=15)
    
    # Split signal into five second chunks
    sig_splits = []
    for i in range(0, len(sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):
        split = sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]

        # End of signal?
        if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):
            break
        
        sig_splits.append(split)
        
    # Extract mel spectrograms for each audio chunk
    s_cnt = 0
    saved_samples = []
    for chunk in sig_splits:
        
        hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))
        mel_spec = librosa.feature.melspectrogram(y=chunk, 
                                                  sr=SAMPLE_RATE, 
                                                  n_fft=1024, 
                                                  hop_length=hop_length, 
                                                  n_mels=SPEC_SHAPE[0], 
                                                  fmin=FMIN, 
                                                  fmax=FMAX)
    
        mel_spec = librosa.power_to_db(mel_spec, ref=np.max) 
        
        # Normalize
        mel_spec -= mel_spec.min()
        mel_spec /= mel_spec.max()
        
        # Save as image file
        save_dir = os.path.join(output_dir, primary_label)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        save_path = os.path.join(save_dir, filepath.rsplit(os.sep, 1)[-1].rsplit('.', 1)[0] + 
                                 '_' + str(s_cnt) + '.png')
        im = Image.fromarray(mel_spec * 255.0).convert("L")
        im.save(save_path)
        
        saved_samples.append(save_path)
        s_cnt += 1
        
        
    return saved_samples

print('FINAL NUMBER OF AUDIO FILES IN TRAINING DATA:', len(TRAIN))
FINAL NUMBER OF AUDIO FILES IN TRAINING DATA: 10955
# Parse audio files and extract training samples
input_dir = '../input/birdclef-2021/train_short_audio/'
output_dir = '../working/melspectrogram_dataset/'
samples = []
with tqdm(total=len(TRAIN)) as pbar:
    for idx, row in TRAIN.iterrows():
        pbar.update(1)
        
        if row.primary_label in most_represented_birds:
            audio_file_path = os.path.join(input_dir, row.primary_label, row.filename)
            samples += get_spectrograms(audio_file_path, row.primary_label, output_dir)
            
TRAIN_SPECS = shuffle(samples, random_state=RANDOM_SEED)
print('SUCCESSFULLY EXTRACTED {} SPECTROGRAMS'.format(len(TRAIN_SPECS)))
100%|██████████| 10955/10955 [12:20<00:00, 14.79it/s]
SUCCESSFULLY EXTRACTED 30322 SPECTROGRAMS
Nice! These are good samples. Notice how some of them only contain a fraction of a bird call? That's an issue we won't deal with in this tutorial. We will simply ignore the fact that samples might not contain any bird sounds.

4. Load training samples
For now, our spectrograms reside in a working directory. If we want to train a model, we have to load them into memory. Yet, with potentially hundreds of thousands of extracted spectrograms, an in-memory dataset is not a good idea. But for now, loading samples from disk and combining them into a large NumPy array is fine. It’s the easiest way to use these data for training with Keras.

# Parse all samples and add spectrograms into train data, primary_labels into label data
train_specs, train_labels = [], []
with tqdm(total=len(TRAIN_SPECS)) as pbar:
    for path in TRAIN_SPECS:
        pbar.update(1)

        # Open image
        spec = Image.open(path)

        # Convert to numpy array
        spec = np.array(spec, dtype='float32')
        
        # Normalize between 0.0 and 1.0
        # and exclude samples with nan 
        spec -= spec.min()
        spec /= spec.max()
        if not spec.max() == 1.0 or not spec.min() == 0.0:
            continue

        # Add channel axis to 2D array
        spec = np.expand_dims(spec, -1)

        # Add new dimension for batch size
        spec = np.expand_dims(spec, 0)

        # Add to train data
        if len(train_specs) == 0:
            train_specs = spec
        else:
            train_specs = np.vstack((train_specs, spec))

        # Add to label data
        target = np.zeros((len(LABELS)), dtype='float32')
        bird = path.split(os.sep)[-2]
        target[LABELS.index(bird)] = 1.0
        if len(train_labels) == 0:
            train_labels = target
        else:
            train_labels = np.vstack((train_labels, target))
100%|██████████| 30322/30322 [1:13:13<00:00,  6.90it/s]
5. Build a simple model
f1_score=F1Score(num_classes=len(LABELS),average='macro',name='f1_score')
Model 1
# Make sure your experiments are reproducible
tf.random.set_seed(RANDOM_SEED)

# Build a simple model as a sequence of  convolutional blocks.
# Each block has the sequence CONV --> RELU --> BNORM --> MAXPOOL.
# Finally, perform global average pooling and add 2 dense layers.
# The last layer is our classification layer and is softmax activated.
# (Well it's a multi-label task so sigmoid might actually be a better choice)
model = tf.keras.Sequential([
    
    # First conv block
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', 
                           input_shape=(SPEC_SHAPE[0], SPEC_SHAPE[1], 1)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),
    
    # Second conv block
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)), 
    
    # Third conv block
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)), 
    
    # Fourth conv block
    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),
        
    
    
#     # Global pooling instead of flatten()
    tf.keras.layers.GlobalAveragePooling2D(), 
    
    # Dense block
    tf.keras.layers.Dense(256, activation='relu'),  
    tf.keras.layers.Dropout(0.5),
#     tf.keras.layers.Dense(64, activation='relu'),   
#     tf.keras.layers.Dropout(0.5),
    
    # Classification layer
    tf.keras.layers.Dense(len(LABELS), activation='softmax')
])



print('MODEL HAS {} PARAMETERS.'.format(model.count_params()))
MODEL HAS 414760 PARAMETERS.
# Compile the model and specify optimizer, loss and metric
model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.01),
              metrics=['accuracy',f1_score])
# Add callbacks to reduce the learning rate if needed, early stopping, and checkpoint saving
callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_f1_score', 
                                                  patience=2, 
                                                  verbose=1, 
                                                  mode='max',
                                                  factor=0.5),
             tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                                              verbose=1,
                                              patience=20),
             tf.keras.callbacks.ModelCheckpoint(filepath='./best_model.h5', 
                                                monitor='val_f1_score',
                                                mode='max',
                                                verbose=1,
                                                save_best_only=True)]
def plot_history(history):
    his=pd.DataFrame(history.history)
    plt.subplots(1,2,figsize=(16,8))
    
    #loss:
    plt.subplot(1,2,1)
    plt.plot(range(len(his)),his['loss'],color='g',label='training')
    plt.plot(range(len(his)),his['val_loss'],color='r',label='validation')
    plt.legend()
    plt.title('Loss')
    
    #accuracy
    plt.subplot(1,2,2)
    plt.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')
    plt.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')
    
#     #f1_score
#     plt.plot(range(len(his)),his['f1_score'],color='steelblue',label='training_f1')
#     plt.plot(range(len(his)),his['val_f1_score'],color='maroon',label='validation_f1')
    
    plt.legend()
    plt.title('accuracy')
    
    plt.show()              
TRAINING
# Let's train the model for a few epochs
his=model.fit(train_specs, 
          train_labels,
          batch_size=32,
          validation_split=0.2,
          callbacks=callbacks,
          epochs=EPOCHS)

plot_history(his)
Epoch 1/50
758/758 [==============================] - 10s 8ms/step - loss: 3.2132 - accuracy: 0.1856 - f1_score: 0.1505 - val_loss: 2.4331 - val_accuracy: 0.3503 - val_f1_score: 0.3130

Epoch 00001: val_f1_score improved from -inf to 0.31302, saving model to ./best_model.h5
Epoch 2/50
758/758 [==============================] - 5s 7ms/step - loss: 2.0155 - accuracy: 0.4758 - f1_score: 0.4537 - val_loss: 1.6440 - val_accuracy: 0.5755 - val_f1_score: 0.5563

Epoch 00002: val_f1_score improved from 0.31302 to 0.55628, saving model to ./best_model.h5
Epoch 3/50
758/758 [==============================] - 5s 7ms/step - loss: 1.6212 - accuracy: 0.5778 - f1_score: 0.5639 - val_loss: 1.6003 - val_accuracy: 0.5937 - val_f1_score: 0.6009

Epoch 00003: val_f1_score improved from 0.55628 to 0.60093, saving model to ./best_model.h5
Epoch 4/50
758/758 [==============================] - 5s 7ms/step - loss: 1.4276 - accuracy: 0.6321 - f1_score: 0.6199 - val_loss: 1.4663 - val_accuracy: 0.6295 - val_f1_score: 0.6279

Epoch 00004: val_f1_score improved from 0.60093 to 0.62785, saving model to ./best_model.h5
Epoch 5/50
758/758 [==============================] - 6s 8ms/step - loss: 1.2861 - accuracy: 0.6749 - f1_score: 0.6652 - val_loss: 1.4311 - val_accuracy: 0.6403 - val_f1_score: 0.6398

Epoch 00005: val_f1_score improved from 0.62785 to 0.63984, saving model to ./best_model.h5
Epoch 6/50
758/758 [==============================] - 5s 7ms/step - loss: 1.1766 - accuracy: 0.7023 - f1_score: 0.6922 - val_loss: 1.3875 - val_accuracy: 0.6586 - val_f1_score: 0.6541

Epoch 00006: val_f1_score improved from 0.63984 to 0.65406, saving model to ./best_model.h5
Epoch 7/50
758/758 [==============================] - 5s 7ms/step - loss: 1.0787 - accuracy: 0.7295 - f1_score: 0.7199 - val_loss: 1.5560 - val_accuracy: 0.6111 - val_f1_score: 0.6108

Epoch 00007: val_f1_score did not improve from 0.65406
Epoch 8/50
758/758 [==============================] - 5s 7ms/step - loss: 0.9622 - accuracy: 0.7561 - f1_score: 0.7476 - val_loss: 1.2695 - val_accuracy: 0.6903 - val_f1_score: 0.6878

Epoch 00008: val_f1_score improved from 0.65406 to 0.68781, saving model to ./best_model.h5
Epoch 9/50
758/758 [==============================] - 5s 7ms/step - loss: 0.9002 - accuracy: 0.7754 - f1_score: 0.7661 - val_loss: 1.2843 - val_accuracy: 0.6982 - val_f1_score: 0.6934

Epoch 00009: val_f1_score improved from 0.68781 to 0.69336, saving model to ./best_model.h5
Epoch 10/50
758/758 [==============================] - 5s 7ms/step - loss: 0.8125 - accuracy: 0.7981 - f1_score: 0.7890 - val_loss: 1.3370 - val_accuracy: 0.6806 - val_f1_score: 0.6735

Epoch 00010: val_f1_score did not improve from 0.69336
Epoch 11/50
758/758 [==============================] - 5s 7ms/step - loss: 0.7433 - accuracy: 0.8197 - f1_score: 0.8132 - val_loss: 1.4166 - val_accuracy: 0.6893 - val_f1_score: 0.6860

Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

Epoch 00011: val_f1_score did not improve from 0.69336
Epoch 12/50
758/758 [==============================] - 5s 7ms/step - loss: 0.6113 - accuracy: 0.8576 - f1_score: 0.8487 - val_loss: 1.2546 - val_accuracy: 0.7218 - val_f1_score: 0.7198

Epoch 00012: val_f1_score improved from 0.69336 to 0.71976, saving model to ./best_model.h5
Epoch 13/50
758/758 [==============================] - 5s 7ms/step - loss: 0.5085 - accuracy: 0.8898 - f1_score: 0.8846 - val_loss: 1.2637 - val_accuracy: 0.7182 - val_f1_score: 0.7132

Epoch 00013: val_f1_score did not improve from 0.71976
Epoch 14/50
758/758 [==============================] - 5s 7ms/step - loss: 0.4614 - accuracy: 0.9055 - f1_score: 0.9010 - val_loss: 1.3067 - val_accuracy: 0.7127 - val_f1_score: 0.7120

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.

Epoch 00014: val_f1_score did not improve from 0.71976
Epoch 15/50
758/758 [==============================] - 6s 7ms/step - loss: 0.3994 - accuracy: 0.9251 - f1_score: 0.9198 - val_loss: 1.2596 - val_accuracy: 0.7324 - val_f1_score: 0.7283

Epoch 00015: val_f1_score improved from 0.71976 to 0.72831, saving model to ./best_model.h5
Epoch 16/50
758/758 [==============================] - 5s 7ms/step - loss: 0.3488 - accuracy: 0.9428 - f1_score: 0.9386 - val_loss: 1.3053 - val_accuracy: 0.7279 - val_f1_score: 0.7252

Epoch 00016: val_f1_score did not improve from 0.72831
Epoch 17/50
758/758 [==============================] - 5s 7ms/step - loss: 0.3270 - accuracy: 0.9490 - f1_score: 0.9433 - val_loss: 1.3050 - val_accuracy: 0.7314 - val_f1_score: 0.7260

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.

Epoch 00017: val_f1_score did not improve from 0.72831
Epoch 18/50
758/758 [==============================] - 6s 7ms/step - loss: 0.3050 - accuracy: 0.9574 - f1_score: 0.9539 - val_loss: 1.3014 - val_accuracy: 0.7259 - val_f1_score: 0.7208

Epoch 00018: val_f1_score did not improve from 0.72831
Epoch 19/50
758/758 [==============================] - 5s 7ms/step - loss: 0.2879 - accuracy: 0.9623 - f1_score: 0.9593 - val_loss: 1.2974 - val_accuracy: 0.7300 - val_f1_score: 0.7241

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.

Epoch 00019: val_f1_score did not improve from 0.72831
Epoch 20/50
758/758 [==============================] - 5s 7ms/step - loss: 0.2705 - accuracy: 0.9695 - f1_score: 0.9667 - val_loss: 1.2961 - val_accuracy: 0.7317 - val_f1_score: 0.7255

Epoch 00020: val_f1_score did not improve from 0.72831
Epoch 21/50
758/758 [==============================] - 6s 7ms/step - loss: 0.2617 - accuracy: 0.9712 - f1_score: 0.9686 - val_loss: 1.3128 - val_accuracy: 0.7332 - val_f1_score: 0.7274

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.

Epoch 00021: val_f1_score did not improve from 0.72831
Epoch 22/50
758/758 [==============================] - 6s 8ms/step - loss: 0.2610 - accuracy: 0.9696 - f1_score: 0.9664 - val_loss: 1.3092 - val_accuracy: 0.7307 - val_f1_score: 0.7253

Epoch 00022: val_f1_score did not improve from 0.72831
Epoch 23/50
758/758 [==============================] - 6s 8ms/step - loss: 0.2526 - accuracy: 0.9757 - f1_score: 0.9723 - val_loss: 1.3091 - val_accuracy: 0.7312 - val_f1_score: 0.7250

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.

Epoch 00023: val_f1_score did not improve from 0.72831
Epoch 24/50
758/758 [==============================] - 6s 8ms/step - loss: 0.2500 - accuracy: 0.9755 - f1_score: 0.9720 - val_loss: 1.3028 - val_accuracy: 0.7345 - val_f1_score: 0.7284

Epoch 00024: val_f1_score improved from 0.72831 to 0.72836, saving model to ./best_model.h5
Epoch 25/50
758/758 [==============================] - 5s 7ms/step - loss: 0.2451 - accuracy: 0.9768 - f1_score: 0.9734 - val_loss: 1.3097 - val_accuracy: 0.7333 - val_f1_score: 0.7270

Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.

Epoch 00025: val_f1_score did not improve from 0.72836
Epoch 26/50
758/758 [==============================] - 6s 7ms/step - loss: 0.2436 - accuracy: 0.9785 - f1_score: 0.9752 - val_loss: 1.3089 - val_accuracy: 0.7333 - val_f1_score: 0.7277

Epoch 00026: val_f1_score did not improve from 0.72836
Epoch 27/50
758/758 [==============================] - 5s 7ms/step - loss: 0.2448 - accuracy: 0.9763 - f1_score: 0.9727 - val_loss: 1.3068 - val_accuracy: 0.7324 - val_f1_score: 0.7261

Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.

Epoch 00027: val_f1_score did not improve from 0.72836
Epoch 28/50
758/758 [==============================] - 5s 7ms/step - loss: 0.2421 - accuracy: 0.9769 - f1_score: 0.9739 - val_loss: 1.3066 - val_accuracy: 0.7337 - val_f1_score: 0.7277

Epoch 00028: val_f1_score did not improve from 0.72836
Epoch 29/50
758/758 [==============================] - 5s 7ms/step - loss: 0.2443 - accuracy: 0.9783 - f1_score: 0.9748 - val_loss: 1.3084 - val_accuracy: 0.7330 - val_f1_score: 0.7272

Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.

Epoch 00029: val_f1_score did not improve from 0.72836
Epoch 30/50
758/758 [==============================] - 5s 7ms/step - loss: 0.2419 - accuracy: 0.9774 - f1_score: 0.9748 - val_loss: 1.3096 - val_accuracy: 0.7315 - val_f1_score: 0.7260

Epoch 00030: val_f1_score did not improve from 0.72836
Epoch 31/50
758/758 [==============================] - 5s 7ms/step - loss: 0.2453 - accuracy: 0.9770 - f1_score: 0.9732 - val_loss: 1.3098 - val_accuracy: 0.7335 - val_f1_score: 0.7278

Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.

Epoch 00031: val_f1_score did not improve from 0.72836
Epoch 32/50
758/758 [==============================] - 6s 7ms/step - loss: 0.2434 - accuracy: 0.9766 - f1_score: 0.9738 - val_loss: 1.3113 - val_accuracy: 0.7330 - val_f1_score: 0.7276

Epoch 00032: val_f1_score did not improve from 0.72836
Epoch 00032: early stopping

model.save('./my_model_1.h5')
Model 2
# Make sure your experiments are reproducible
tf.random.set_seed(RANDOM_SEED)

# Build a simple model as a sequence of  convolutional blocks.
# Each block has the sequence CONV --> RELU --> BNORM --> MAXPOOL.
# Finally, perform global average pooling and add 2 dense layers.
# The last layer is our classification layer and is softmax activated.
# (Well it's a multi-label task so sigmoid might actually be a better choice)
model2 = tf.keras.Sequential([
    
    # First conv block
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', 
                           input_shape=(SPEC_SHAPE[0], SPEC_SHAPE[1], 1)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),
    
    # Second conv block
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)), 
    
    # Third conv block
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)), 
    
    # Fourth conv block
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),
        
    
    
#     # Global pooling instead of flatten()
    tf.keras.layers.GlobalAveragePooling2D(), 
    
    # Dense block
    tf.keras.layers.Dense(128, activation='relu'),  
    tf.keras.layers.Dropout(0.5),
#     tf.keras.layers.Dense(64, activation='relu'),   
#     tf.keras.layers.Dropout(0.5),
    
    # Classification layer
    tf.keras.layers.Dense(len(LABELS), activation='softmax')
])



print('MODEL HAS {} PARAMETERS.'.format(model2.count_params()))
MODEL HAS 119784 PARAMETERS.
# Compile the model and specify optimizer, loss and metric
model2.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.01),
              metrics=['accuracy',f1_score])

# Add callbacks to reduce the learning rate if needed, early stopping, and checkpoint saving
callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_f1_score',
                                                  mode='max',
                                                  patience=2, 
                                                  verbose=1, 
                                                  factor=0.5),
             tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                                              verbose=1,
                                              patience=20),
             tf.keras.callbacks.ModelCheckpoint(filepath='best_model2.h5', 
                                                mode='max',
                                                monitor='val_f1_score',
                                                verbose=0,
                                                save_best_only=True)]
# Let's train the model for a few epochs
his2=model2.fit(train_specs, 
          train_labels,
          batch_size=32,
          validation_split=0.2,
          callbacks=callbacks,
          verbose=1,
          epochs=EPOCHS)

plot_history(his2)
Epoch 1/50
758/758 [==============================] - 7s 8ms/step - loss: 3.2533 - accuracy: 0.1598 - f1_score: 0.3875 - val_loss: 2.2979 - val_accuracy: 0.4081 - val_f1_score: 0.3633
Epoch 2/50
758/758 [==============================] - 5s 7ms/step - loss: 2.2221 - accuracy: 0.4179 - f1_score: 0.3848 - val_loss: 2.2123 - val_accuracy: 0.4383 - val_f1_score: 0.4207
Epoch 3/50
758/758 [==============================] - 5s 7ms/step - loss: 1.8610 - accuracy: 0.5166 - f1_score: 0.4971 - val_loss: 2.0615 - val_accuracy: 0.4721 - val_f1_score: 0.4631
Epoch 4/50
758/758 [==============================] - 5s 6ms/step - loss: 1.6759 - accuracy: 0.5660 - f1_score: 0.5515 - val_loss: 1.5830 - val_accuracy: 0.6041 - val_f1_score: 0.6021
Epoch 5/50
758/758 [==============================] - 5s 7ms/step - loss: 1.5320 - accuracy: 0.6037 - f1_score: 0.5902 - val_loss: 1.6561 - val_accuracy: 0.5800 - val_f1_score: 0.5769
Epoch 6/50
758/758 [==============================] - 5s 6ms/step - loss: 1.4558 - accuracy: 0.6248 - f1_score: 0.6129 - val_loss: 1.4706 - val_accuracy: 0.6268 - val_f1_score: 0.6177
Epoch 7/50
758/758 [==============================] - 5s 7ms/step - loss: 1.3561 - accuracy: 0.6483 - f1_score: 0.6365 - val_loss: 1.4982 - val_accuracy: 0.6253 - val_f1_score: 0.6247
Epoch 8/50
758/758 [==============================] - 5s 7ms/step - loss: 1.2758 - accuracy: 0.6715 - f1_score: 0.6616 - val_loss: 1.4345 - val_accuracy: 0.6433 - val_f1_score: 0.6358
Epoch 9/50
758/758 [==============================] - 5s 7ms/step - loss: 1.2374 - accuracy: 0.6844 - f1_score: 0.6743 - val_loss: 1.3545 - val_accuracy: 0.6642 - val_f1_score: 0.6593
Epoch 10/50
758/758 [==============================] - 5s 6ms/step - loss: 1.1838 - accuracy: 0.6974 - f1_score: 0.6886 - val_loss: 1.3998 - val_accuracy: 0.6463 - val_f1_score: 0.6454
Epoch 11/50
758/758 [==============================] - 5s 7ms/step - loss: 1.1173 - accuracy: 0.7157 - f1_score: 0.7077 - val_loss: 1.3616 - val_accuracy: 0.6577 - val_f1_score: 0.6497

Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 12/50
758/758 [==============================] - 5s 6ms/step - loss: 0.9939 - accuracy: 0.7497 - f1_score: 0.7385 - val_loss: 1.2992 - val_accuracy: 0.6887 - val_f1_score: 0.6839
Epoch 13/50
758/758 [==============================] - 5s 6ms/step - loss: 0.9333 - accuracy: 0.7645 - f1_score: 0.7554 - val_loss: 1.2911 - val_accuracy: 0.6901 - val_f1_score: 0.6824
Epoch 14/50
758/758 [==============================] - 6s 7ms/step - loss: 0.8837 - accuracy: 0.7792 - f1_score: 0.7697 - val_loss: 1.2950 - val_accuracy: 0.6859 - val_f1_score: 0.6789

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 15/50
758/758 [==============================] - 5s 7ms/step - loss: 0.8270 - accuracy: 0.7954 - f1_score: 0.7867 - val_loss: 1.2583 - val_accuracy: 0.7038 - val_f1_score: 0.6976
Epoch 16/50
758/758 [==============================] - 5s 7ms/step - loss: 0.7854 - accuracy: 0.8067 - f1_score: 0.7970 - val_loss: 1.2852 - val_accuracy: 0.7009 - val_f1_score: 0.6958
Epoch 17/50
758/758 [==============================] - 5s 6ms/step - loss: 0.7647 - accuracy: 0.8135 - f1_score: 0.8030 - val_loss: 1.2756 - val_accuracy: 0.7038 - val_f1_score: 0.6984
Epoch 18/50
758/758 [==============================] - 5s 7ms/step - loss: 0.7445 - accuracy: 0.8196 - f1_score: 0.8109 - val_loss: 1.2856 - val_accuracy: 0.7004 - val_f1_score: 0.6949
Epoch 19/50
758/758 [==============================] - 5s 6ms/step - loss: 0.7159 - accuracy: 0.8261 - f1_score: 0.8171 - val_loss: 1.2841 - val_accuracy: 0.7007 - val_f1_score: 0.6971

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 20/50
758/758 [==============================] - 5s 7ms/step - loss: 0.6886 - accuracy: 0.8365 - f1_score: 0.8291 - val_loss: 1.2916 - val_accuracy: 0.7078 - val_f1_score: 0.7022
Epoch 21/50
758/758 [==============================] - 5s 6ms/step - loss: 0.6708 - accuracy: 0.8412 - f1_score: 0.8336 - val_loss: 1.3088 - val_accuracy: 0.7047 - val_f1_score: 0.7004
Epoch 22/50
758/758 [==============================] - 6s 7ms/step - loss: 0.6401 - accuracy: 0.8509 - f1_score: 0.8426 - val_loss: 1.3013 - val_accuracy: 0.7068 - val_f1_score: 0.7009

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 23/50
758/758 [==============================] - 5s 7ms/step - loss: 0.6424 - accuracy: 0.8515 - f1_score: 0.8437 - val_loss: 1.2974 - val_accuracy: 0.7058 - val_f1_score: 0.7016
Epoch 24/50
758/758 [==============================] - 7s 9ms/step - loss: 0.6252 - accuracy: 0.8578 - f1_score: 0.8490 - val_loss: 1.3011 - val_accuracy: 0.7101 - val_f1_score: 0.7050
Epoch 25/50
758/758 [==============================] - 6s 8ms/step - loss: 0.6257 - accuracy: 0.8574 - f1_score: 0.8504 - val_loss: 1.3028 - val_accuracy: 0.7071 - val_f1_score: 0.7018
Epoch 26/50
758/758 [==============================] - 6s 8ms/step - loss: 0.6158 - accuracy: 0.8610 - f1_score: 0.8535 - val_loss: 1.3208 - val_accuracy: 0.7079 - val_f1_score: 0.7031

Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
Epoch 27/50
758/758 [==============================] - 5s 7ms/step - loss: 0.6140 - accuracy: 0.8567 - f1_score: 0.8481 - val_loss: 1.3108 - val_accuracy: 0.7075 - val_f1_score: 0.7032
Epoch 28/50
758/758 [==============================] - 5s 7ms/step - loss: 0.5996 - accuracy: 0.8584 - f1_score: 0.8495 - val_loss: 1.3168 - val_accuracy: 0.7063 - val_f1_score: 0.7012

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
Epoch 29/50
758/758 [==============================] - 5s 6ms/step - loss: 0.6010 - accuracy: 0.8603 - f1_score: 0.8512 - val_loss: 1.3180 - val_accuracy: 0.7066 - val_f1_score: 0.7017
Epoch 30/50
758/758 [==============================] - 6s 7ms/step - loss: 0.5906 - accuracy: 0.8656 - f1_score: 0.8589 - val_loss: 1.3153 - val_accuracy: 0.7066 - val_f1_score: 0.7025

Epoch 00030: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
Epoch 31/50
758/758 [==============================] - 5s 6ms/step - loss: 0.5941 - accuracy: 0.8666 - f1_score: 0.8573 - val_loss: 1.3202 - val_accuracy: 0.7078 - val_f1_score: 0.7045
Epoch 32/50
758/758 [==============================] - 5s 7ms/step - loss: 0.5974 - accuracy: 0.8629 - f1_score: 0.8565 - val_loss: 1.3180 - val_accuracy: 0.7066 - val_f1_score: 0.7018

Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.
Epoch 33/50
758/758 [==============================] - 5s 6ms/step - loss: 0.5922 - accuracy: 0.8670 - f1_score: 0.8586 - val_loss: 1.3187 - val_accuracy: 0.7070 - val_f1_score: 0.7023
Epoch 34/50
758/758 [==============================] - 5s 6ms/step - loss: 0.5970 - accuracy: 0.8645 - f1_score: 0.8555 - val_loss: 1.3181 - val_accuracy: 0.7083 - val_f1_score: 0.7036

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
Epoch 35/50
758/758 [==============================] - 5s 7ms/step - loss: 0.6011 - accuracy: 0.8628 - f1_score: 0.8552 - val_loss: 1.3188 - val_accuracy: 0.7075 - val_f1_score: 0.7028
Epoch 00035: early stopping

model2.save('./my_model_2.h5')
6. Soundscape analysis
Soundscape_analysis1

# Load the best checkpoint
model = tf.keras.models.load_model('./my_model_1.h5')
#model2= tf.keras.models.load_model('./my_best_model2.h5')
model2= tf.keras.models.load_model('./best_model.h5')

# Pick a soundscape
#soundscape_path = '../input/birdclef-2021/train_short_audio/mallar3/XC133448.ogg'
#soundscape_path='../input/birdclef-2021/train_short_audio/mallar3/XC140460.ogg'
#soundscape_path='../input/birdclef-2021/train_short_audio/sonspa/XC125723.ogg'
#soundscape_path='../input/birdclef-2021/train_short_audio/houfin/XC163052.ogg'
#soundscape_path='../input/birdclef-2021/train_short_audio/ducfly/XC123767.ogg'
soundscape_path='../input/birdclef-2021/train_short_audio/ducfly/XC123766.ogg'

# Open it with librosa
sig, rate = librosa.load(soundscape_path, sr=SAMPLE_RATE)

# Store results so that we can analyze them later
data = {'row_id': [], 'prediction': [], 'score': []}

# Split signal into 5-second chunks
# Just like we did before (well, this could actually be a seperate function)
sig_splits = []
for i in range(0, len(sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):
    split = sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]

    # End of signal?
    if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):
        break

    sig_splits.append(split)
    
# Get the spectrograms and run inference on each of them
# This should be the exact same process as we used to
# generate training samples!
seconds, scnt = 0, 0
for chunk in sig_splits:
    
    # Keep track of the end time of each chunk
    seconds += 5
        
    # Get the spectrogram
    hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))
    mel_spec = librosa.feature.melspectrogram(y=chunk, 
                                              sr=SAMPLE_RATE, 
                                              n_fft=1024, 
                                              hop_length=hop_length, 
                                              n_mels=SPEC_SHAPE[0], 
                                              fmin=FMIN, 
                                              fmax=FMAX)

    mel_spec = librosa.power_to_db(mel_spec, ref=np.max) 

    # Normalize to match the value range we used during training.
    # That's something you should always double check!
    mel_spec -= mel_spec.min()
    mel_spec /= mel_spec.max()
    
    # Add channel axis to 2D array
    mel_spec = np.expand_dims(mel_spec, -1)

    # Add new dimension for batch size
    mel_spec = np.expand_dims(mel_spec, 0)
    
    # Predict
    p = model.predict(mel_spec)[0] + model2.predict(mel_spec)[0] 
    
    
    # Get highest scoring species
    idx = p.argmax()
    species = LABELS[idx]
    score = p[idx]
    
    # Prepare submission entry
    data['row_id'].append(soundscape_path.split(os.sep)[-1].rsplit('_', 1)[0] + 
                          '_' + str(seconds))    
    
    # Decide if it's a "nocall" or a species by applying a threshold
    if score > 0.4:
        data['prediction'].append(species)
        scnt += 1
    else:
        data['prediction'].append('nocall')
        
    # Add the confidence score as well
    data['score'].append(score)
        
print('SOUNSCAPE ANALYSIS DONE. FOUND {} BIRDS.'.format(scnt))
print( species)
SOUNSCAPE ANALYSIS DONE. FOUND 16 BIRDS.
ducfly
# Make a new data frame
results = pd.DataFrame(data, columns = ['row_id', 'prediction', 'score'])

# Merge with ground truth so we can inspect
gt = pd.read_csv('../input/birdclef-2021/train_soundscape_labels.csv',)
results = pd.merge(gt, results, on='row_id')

# Let's look at the first 50 entries
results.head(50)
row_id	site	audio_id	seconds	birds	prediction	score
Soundscape_analysis2_2

 
 
 
 
